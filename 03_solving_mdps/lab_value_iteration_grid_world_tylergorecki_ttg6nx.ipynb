{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c",
      "metadata": {
        "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c"
      },
      "source": [
        "### Lab: Value Iteration in a Grid World\n",
        "\n",
        "### University of Virginia\n",
        "### Reinforcement Learning\n",
        "#### Last updated: December 11, 2023\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45",
      "metadata": {
        "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45"
      },
      "source": [
        "#### Instructions:\n",
        "\n",
        "Implement value iteration for a $4 \\times 3$ gridworld environment. This will measure the value of each state. A robot in this world can make discrete moves: one step up, down, left or right. These actions are deterministic, meaning that the action selected will be taken with probability 1. There is a terminal state with reward +1 in the bottom right corner. All other states have reward 0. The discount factor is 0.9. Use tolerance $\\theta=0.01$. Show all code and results.\n",
        "\n",
        "**Note**: Do not use libraries from `networkx`, `gym`, `gymnasium` when solving this problem.\n",
        "\n",
        "#### Total Points: 12"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5956322-1af2-4a18-b65c-d98dc08454c8",
      "metadata": {
        "id": "a5956322-1af2-4a18-b65c-d98dc08454c8"
      },
      "source": [
        "---\n",
        "\n",
        "#### 1) **(POINTS: 2)** As part of your solution, create a GridWorld class with these attributes:\n",
        "\n",
        "- `nrows` : number of rows in the grid\n",
        "- `ncols` : number of columns in the grid\n",
        "\n",
        "and these methods:\n",
        "\n",
        "- `value_iteration()` with behavior described in [2] below\n",
        "- `get_reward()` : given the agent row and column, return the reward\n",
        "\n",
        "The class may include additional attributes and methods as well.\n",
        "\n",
        "Create an instance using the class, and call `nrows`, `ncols`, and `get_reward()` to verify correctness.\n",
        "\n",
        "You will not be graded on the implementation of `value_iteration()` for this problem.\n",
        "\n",
        "#### 2) **(POINTS: 8)** Here, you will be graded on the implementation of `value_iteration()`.\n",
        "Call `value_iteration()` to calculate and return the value function array. For each sweep over the states, have the function print out the intermediate array.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd213107-d461-431c-b1e1-d1d3c099976d",
      "metadata": {
        "id": "bd213107-d461-431c-b1e1-d1d3c099976d"
      },
      "source": [
        "#### Enter all code here (you may also use multiple cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "338948a3-db77-4458-a6ed-a3f5e4aa3c72",
      "metadata": {
        "id": "338948a3-db77-4458-a6ed-a3f5e4aa3c72"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "858c5f68-7b54-459e-95f3-59697cfa6d23",
      "metadata": {
        "id": "858c5f68-7b54-459e-95f3-59697cfa6d23"
      },
      "source": [
        "#### 1) Create and test the class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "a21a43af-2be7-49d3-aec0-1934160bb61c",
      "metadata": {
        "id": "a21a43af-2be7-49d3-aec0-1934160bb61c"
      },
      "outputs": [],
      "source": [
        "class GridWorld():\n",
        "    def __init__(self, nrows, ncols):\n",
        "        self.nrows = nrows\n",
        "        self.ncols = ncols\n",
        "        self.values = np.zeros((nrows, ncols))\n",
        "        self.actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "    def value_iteration(self, theta=0.01, discount_factor=0.9, max_iter = 10):\n",
        "      while True:\n",
        "        max_delta = 0\n",
        "        v = np.copy(self.values)\n",
        "\n",
        "        for i in range(self.nrows):\n",
        "          for j in range(self.ncols):\n",
        "            # terminal state won't have value\n",
        "            if i == self.nrows - 1 and j == self.ncols - 1:\n",
        "              v[i,j] = 0\n",
        "              continue\n",
        "\n",
        "            # loop through all actions, finding best one from each state\n",
        "            action_values = []\n",
        "            for action in self.actions:\n",
        "              if action == 'up':\n",
        "                next_i = i - 1\n",
        "                next_j = j\n",
        "              elif action == 'down':\n",
        "                next_i = i + 1\n",
        "                next_j = j\n",
        "              elif action == 'left':\n",
        "                next_i = i\n",
        "                next_j = j - 1\n",
        "              elif action == 'right':\n",
        "                next_i = i\n",
        "                next_j = j + 1\n",
        "\n",
        "              # check if action goes outside the grid, if so stay in state\n",
        "              if next_i < 0 or next_i >= self.nrows or next_j < 0 or next_j >= self.ncols:\n",
        "                action_values.append(self.get_reward(i, j) + discount_factor * v[i,j])\n",
        "              # if next state is within grid, calculate new value\n",
        "              else:\n",
        "                action_values.append(self.get_reward(next_i, next_j) + discount_factor * v[next_i, next_j])\n",
        "\n",
        "            # update state value with max action value\n",
        "            v[i,j] = max(action_values)\n",
        "            max_delta = max(max_delta, abs(v[i,j] - self.values[i,j]))\n",
        "\n",
        "        if max_delta < theta:\n",
        "          print(\"Threshold reached\")\n",
        "          break\n",
        "        elif max_iter == 0:\n",
        "          print(\"Max iterations reached\")\n",
        "          break\n",
        "        else:\n",
        "          max_iter -= 1\n",
        "          self.values = v.copy()\n",
        "          print(\"Iteration: \", 10 - max_iter)\n",
        "          print(self.values)\n",
        "\n",
        "      return self.values\n",
        "\n",
        "    def get_reward(self, row, col):\n",
        "        # if in bottom right, then reward is +1\n",
        "        if row == self.nrows - 1 and col == self.ncols - 1:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gw = GridWorld(4, 3)\n",
        "print(gw.nrows)\n",
        "print(gw.ncols)\n",
        "print(gw.get_reward(0, 0)) # should be no reward\n",
        "print(gw.get_reward(2, 1)) # should be no reward\n",
        "print(gw.get_reward(3, 2)) # bottom right, reward +1"
      ],
      "metadata": {
        "id": "dx8ZriYQgn87",
        "outputId": "0a3c2fc7-8ecf-4c95-d4bf-3a46fa9ca70b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dx8ZriYQgn87",
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "3\n",
            "0\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb15497-6f08-4f3e-abef-43099f05fba7",
      "metadata": {
        "id": "dfb15497-6f08-4f3e-abef-43099f05fba7"
      },
      "source": [
        "#### 2) Run value iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "922da11f-c12f-4535-8a8a-63cbcab3e736",
      "metadata": {
        "id": "922da11f-c12f-4535-8a8a-63cbcab3e736",
        "outputId": "99464f76-f7ae-4b66-dabc-6af8666b4afb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  1\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n",
            "Iteration:  2\n",
            "[[0.  0.  0. ]\n",
            " [0.  0.  0.9]\n",
            " [0.  0.9 1. ]\n",
            " [0.9 1.  0. ]]\n",
            "Iteration:  3\n",
            "[[0.   0.   0.81]\n",
            " [0.   0.81 0.9 ]\n",
            " [0.81 0.9  1.  ]\n",
            " [0.9  1.   0.  ]]\n",
            "Iteration:  4\n",
            "[[0.    0.729 0.81 ]\n",
            " [0.729 0.81  0.9  ]\n",
            " [0.81  0.9   1.   ]\n",
            " [0.9   1.    0.   ]]\n",
            "Iteration:  5\n",
            "[[0.6561 0.729  0.81  ]\n",
            " [0.729  0.81   0.9   ]\n",
            " [0.81   0.9    1.    ]\n",
            " [0.9    1.     0.    ]]\n",
            "Threshold reached\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6561, 0.729 , 0.81  ],\n",
              "       [0.729 , 0.81  , 0.9   ],\n",
              "       [0.81  , 0.9   , 1.    ],\n",
              "       [0.9   , 1.    , 0.    ]])"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ],
      "source": [
        "gw.value_iteration(max_iter=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486efc92-9a71-4956-b877-b6b7cef13031",
      "metadata": {
        "id": "486efc92-9a71-4956-b877-b6b7cef13031"
      },
      "source": [
        "#### 3) **(POINTS: 2)** Based on the value function: After the agent has moved right or down, does it ever make sense for it to backtrack (move up or left)? Explain your reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f4d310a-493c-4bcc-8328-c09c88116b0e",
      "metadata": {
        "id": "5f4d310a-493c-4bcc-8328-c09c88116b0e"
      },
      "source": [
        "After the agent has moved right or down, there is no need for it to ever backtrack. This is due to each value in the resulting value function being greater than all states above or to the left of it. This also makes sense intuitively as the best way to get to the bottom right would always be to move down or to the right, which we are trying to do."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}